import os
import sys
import torch
import shutup
import warnings
from pdf2image import convert_from_path
from transformers import AutoModel, AutoTokenizer


class ChatPDF:
    """
    This class facilitates reading and interacting with a PDF document using a Vision Language Model.
    It converts the PDF to an image, loads a pre-trained model, and allows users to interact with the PDF
    content using text-based queries.
    """

    def __init__(self, path):
        """
        Initializes the ChatPDF class with the PDF path.
        Converts the PDF to an image if not already done, and loads the pre-trained Vision Language Model.
        
        Args:
            path (str): Path to the input PDF file.
        """
        
        self.pdf_path = path
        self.file_name = os.path.splitext(os.path.basename(self.pdf_path))[0]
        self.image = f'../image/{self.file_name}.jpg'
        if os.path.exists(self.image):
            pass
        else:
            self.convert_to_image()

        print('\n' * 2)
        print('Wait while model loads!!!')
        self.print_message()
        print()
        self.load_model()


    def convert_to_image(self):
        """
        Converts the first page of the provided PDF into an image (JPEG format).
        The image is saved to the 'image' folder in the parent directory.
        """
        try:
            print('\n' * 2)
            print('Processing pdf!!!')
            img = convert_from_path(self.pdf_path)
            if not os.path.exists('../image/'):
                os.mkdir('../image/')
            
            image_dir = f'../image/{self.file_name}.jpg'
            img[0].save(image_dir,'JPEG')
            
            print('\n' * 2)
            print('pdf processing complete!!!')

        except Exception as e:
            print('Error Mesage:', e)
    
    def load_model(self):
        """
        Loads the Vision Language Model and its tokenizer.
        The model is loaded with settings optimized for reduced memory usage and 8-bit precision.
        """

        try:
            model_path = 'h2oai/h2ovl-mississippi-2b'
            self.model = AutoModel.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            cache_dir = '../model_tokenizer/model',
            device_map= 'auto',
            load_in_8bit=True,
            ).eval()
    
            print('\n' * 2)
            print('Wait while Tokenizer loads!!!')
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False, cache_dir='../model_tokenizer/tokenizer', max_length= 512)

            
        except Exception as e:
            print('Error Message:', e)
            
    def pdf_qa(self, prompt):
        """
        Interacts with the loaded model by sending a query and receiving a response
        based on the PDF's image content.
        
        Args:
            prompt (str): The question/query to ask the model regarding the PDF content.
        
        Returns:
            str: The response generated by the model.
        """
    
        prompt = f'<image>\n {prompt}'
        generation_config = dict(max_new_tokens=512, do_sample=True)
        response, history = self.model.chat(self.tokenizer, self.image, prompt, generation_config, history=None, return_history=True)
        print('\n' * 2)
        print('chatPDF :', response)

        return response
    
    def print_message(self):
        text = '''
            The Python script relies on Vision Language Model from HuggingFace's repository.
            Inferencing is a time consuming and  cuda dependant task, ensure availability of cuda device.
            The program leverages OCR capability of quantized pre-trained model to  provide answers. 
            The model is not fine-tuned to be use-case specific, it may not always give accurate answers at all time. 
            Use prompt engineering to advantage when facing issues with inaccurate responses. 
        
            Please wait, you will be prompted to ask your question shortly
            
            Type "quit" at anytime, when asked for question, to exit the program.'''
        print('\n' * 2)
        print(text)
        
if __name__ == '__main__':

    
    text_message_1 = '''
        Note :
        Keeping in mind the nature of the task, the program assumes the page count of the pdf to be one.
        Please provide document to start chatting with the pdf!!!'''
    
    while True:
        print(text_message_1)
        print('\n' * 2)
        
        while True:
            pdf_path = input('Enter PDF Path!!! \n')
            if os.path.exists(pdf_path):
                break
            else:
                print("Invalid path. Please enter a valid path.")

        chatpdf = ChatPDF(path=pdf_path)
        
        while True:
            print('\n' * 2)
            prompt = input('Ask your question here: ')
            if prompt.lower() == 'quit':
                break
            else:
                chatpdf.pdf_qa(prompt=prompt)
            
            
